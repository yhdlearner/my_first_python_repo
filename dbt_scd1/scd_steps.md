dbt Production-Grade Generic SCD Type 2 Process with Control Table
This document describes a robust and efficient way to load data into Slowly Changing Dimension Type 2 (SCD Type 2) tables using a generic dbt macro. This approach incorporates a control table for incremental loading windows, ensuring only new or changed data is processed, and minimizing compute and storage costs.

Process Flow Overview

Control Table (control_table): Stores the last_extracted_timestamp (high-watermark) for each source table. This table is updated after a successful dimension load.

Generic Staging Incremental Model (stg_generic_incremental): A macro-driven model that selects only records from a specified raw source that are newer than the last_extracted_timestamp from the control_table.

Generic SCD Type 2 Dimension Macro (scd_type2_merge_macro): This macro encapsulates the core SCD Type 2 logic. It processes the incremental records from the staging layer to identify new and changed records. It then uses a MERGE strategy to:

Expire old versions of changed records (set valid_to and is_current = FALSE).

Insert new records and new versions of changed records (set valid_from, is_current = TRUE, valid_to = NULL).

1. Control Table (control_table.sql)

This table acts as your watermark repository. It will now store watermarks for multiple source tables.

File: models/control/control_table.sql

-- models/control/control_table.sql
-- This model manages the high-watermark for incremental loads for various sources.
-- It should be materialized as a table and updated via a post-hook
-- from the main dimension models using the generic SCD Type 2 macro.

{{ config(
    materialized='table',
    schema='control', -- Or your preferred metadata schema
    full_refresh = false -- Prevent accidental full refresh
) }}

SELECT
    CAST('initial_source' AS STRING) AS source_name, -- Identifier for the source table
    CAST('1970-01-01 00:00:00' AS TIMESTAMP) AS last_extracted_timestamp,
    'initial_load' AS loaded_by,
    CURRENT_TIMESTAMP() AS last_updated_at
WHERE
    FALSE -- Ensures no rows are inserted on initial run, table is created empty

Explanation:

source_name column: Added to differentiate watermarks for different source tables.

WHERE FALSE: On its first run, this query creates an empty table with the defined schema. The actual last_extracted_timestamp will be populated by a post-hook from the generic SCD Type 2 macro.

Initial Value: For the very first run of a specific dimension, you might manually insert a row for that source_name with a very old timestamp or NULL if you want to process all historical data.
INSERT INTO your_project_id.control.control_table (source_name, last_extracted_timestamp, loaded_by, last_updated_at) VALUES ('raw_events', '1970-01-01 00:00:00', 'manual_init', CURRENT_TIMESTAMP());

2. Generic Staging Incremental Model (stg_generic_incremental.sql)

This model will be a generic template that takes source and timestamp column as parameters.

File: models/staging/stg_generic_incremental.sql

-- models/staging/stg_generic_incremental.sql
-- This is a generic staging model that selects incremental records from a source
-- based on a watermark from the control table.
-- It's intended to be called by other models/macros.

-- This model is typically materialized as a view or ephemeral.
-- The actual configuration will be set by the calling model or in dbt_project.yml.

-- This model is not directly runnable, but acts as a template.
-- It will be referenced by the generic SCD Type 2 macro.
-- The `source_name` and `timestamp_column` will be passed dynamically.

-- Example of how the macro will construct the query:
-- SELECT
--     s.*,
--     s.{{ timestamp_column }} AS __dbt_incremental_timestamp
-- FROM
--     {{ source('raw', source_table_name) }} s
-- JOIN
--     {{ ref('control_table') }} ct ON ct.source_name = '{{ source_table_name }}'
-- WHERE
--     s.{{ timestamp_column }} > ct.last_extracted_timestamp

Explanation:

This file itself is more of a placeholder/conceptual model. The actual SQL will be generated by the scd_type2_merge_macro when it constructs the source CTE.

It highlights that the filtering will be dynamic based on the source_name and timestamp_column provided to the macro.

3. Generic SCD Type 2 Dimension Macro (macros/scd_type2_merge_macro.sql)

This is the core of the generic process. It encapsulates all the SCD Type 2 logic.

File: macros/scd_type2_merge_macro.sql

-- macros/scd_type2_merge_macro.sql
-- This macro implements generic Slowly Changing Dimension Type 2 logic.
-- It takes source, key, and change columns as inputs,
-- and manages the control table watermark.

{% macro scd_type2_merge_macro(
    source_table_name,      -- The name of the raw source table (e.g., 'events')
    source_timestamp_column,-- The timestamp column in the source for incremental filtering (e.g., 'last_updated_at')
    key_columns,            -- A LIST of unique business key columns (e.g., ['customer_id', 'product_id'])
    change_columns,         -- A list of columns that trigger a new version if changed (e.g., ['customer_name', 'address', 'email'])
    valid_from_column='valid_from', -- Name of the valid_from column in the target dimension
    valid_to_column='valid_to',     -- Name of the valid_to column in the target dimension
    is_current_column='is_current', -- Name of the is_current column in the target dimension
    control_table_ref='control_table' -- Reference to the control table model
) %}

{{ config(
    materialized='incremental',
    incremental_strategy='merge',
    unique_key=key_columns, # Now a list of columns
    partition_by={
      "field": valid_from_column,
      "data_type": "timestamp",
      "granularity": "day"
    },
    cluster_by=key_columns + [is_current_column], # Cluster by all key columns and is_current
    full_refresh = false,

    # Post-hook to update the control table with the latest processed timestamp for this source
    post_hook="""
        MERGE INTO {{ ref(control_table_ref) }} AS T
        USING (
            SELECT
                '{{ source_table_name }}' AS source_name,
                MAX({{ source_timestamp_column }}) AS max_timestamp
            FROM {{ this }}
            WHERE dbt_batch_id = '{{ run_started_at.strftime("%Y%m%d%H%M%S") }}'
        ) AS S
        ON T.source_name = S.source_name
        WHEN MATCHED THEN
            UPDATE SET last_extracted_timestamp = S.max_timestamp, last_updated_at = CURRENT_TIMESTAMP(), loaded_by = 'dbt_scd2_generic_run'
        WHEN NOT MATCHED THEN
            INSERT (source_name, last_extracted_timestamp, loaded_by, last_updated_at) VALUES (S.source_name, S.max_timestamp, 'dbt_scd2_generic_run', CURRENT_TIMESTAMP());
    """
) }}

WITH source_incremental_data AS (
    -- Select only incremental records from the source based on the control table's watermark
    SELECT
        s.*, -- Select all columns from the source
        s.{{ source_timestamp_column }} AS __dbt_incremental_timestamp -- Alias for consistent use
    FROM
        {{ source('raw', source_table_name) }} s
    JOIN
        {{ ref(control_table_ref) }} ct ON ct.source_name = '{{ source_table_name }}'
    WHERE
        s.{{ source_timestamp_column }} > ct.last_extracted_timestamp
),

-- Calculate a hash for change detection on relevant columns
source_data_with_hash AS (
    SELECT
        *,
        FARM_FINGERPRINT(CONCAT(
            {% for col in change_columns %}
            COALESCE(CAST({{ col }} AS STRING), '__NULL__') {{ '||' if not loop.last else '' }}
            {% endfor %}
        )) AS row_hash
    FROM source_incremental_data
),

-- Identify the latest version of each record within the current batch of source data
latest_source_records AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY {% for col in key_columns %}{{ col }}{{ ', ' if not loop.last }}{% endfor %} ORDER BY __dbt_incremental_timestamp DESC) as rn
    FROM source_data_with_hash
)

SELECT
    {% for col in key_columns %}
    src.{{ col }},
    {% endfor %}
    {% for col in change_columns %}
    src.{{ col }},
    {% endfor %}
    src.__dbt_incremental_timestamp AS event_timestamp, -- Original timestamp from source
    -- For new records or new versions of changed records:
    -- valid_from is the current timestamp of the dbt run
    -- valid_to is NULL (as it's the current active record)
    -- is_current is TRUE
    CURRENT_TIMESTAMP() AS {{ valid_from_column }},
    CAST(NULL AS TIMESTAMP) AS {{ valid_to_column }},
    TRUE AS {{ is_current_column }},
    src.row_hash, -- Store the hash for future change detection
    '{{ run_started_at.strftime("%Y%m%d%H%M%S") }}' AS dbt_batch_id -- Unique ID for this dbt run
FROM
    latest_source_records src
WHERE
    src.rn = 1

{% endmacro %}

Explanation of Changes in scd_type2_merge_macro.sql:

key_columns Parameter: The macro now accepts key_columns as a list (e.g., ['customer_id', 'product_id']).

unique_key=key_columns: In the config block, unique_key is directly assigned the key_columns list. dbt's merge strategy correctly handles a list for the unique_key.

cluster_by=key_columns + [is_current_column]: The cluster_by list is dynamically built by concatenating the key_columns list with the is_current_column.

PARTITION BY in ROW_NUMBER():

PARTITION BY {% for col in key_columns %}{{ col }}{{ ', ' if not loop.last }}{% endfor %}: This Jinja loop iterates through the key_columns list to construct the PARTITION BY clause, ensuring all key columns are used for partitioning the ROW_NUMBER() function.

SELECT statement for final output:

{% for col in key_columns %} loop is used to select all key columns dynamically.

4. Example Generic SCD Type 2 Dimension Model (dim_customers_scd2.sql)

Your dimension model now needs to pass a list for key_columns.

File: models/dimensions/dim_customers_scd2.sql

-- models/dimensions/dim_customers_scd2.sql
-- Calls the generic SCD Type 2 merge macro for the customers dimension.

{{ dbt_production_scd2_process.scd_type2_merge_macro(
    source_table_name='events',               -- The raw source table name (as defined in sources.yml)
    source_timestamp_column='last_updated_at',-- The timestamp column in 'events'
    key_columns=['customer_id'],              -- NOW A LIST: The unique business key for customers
    change_columns=[                          -- List of columns that trigger a new version
        'customer_name',
        'address',
        'city',
        'state',
        'zip_code',
        'email'
    ],
    valid_from_column='valid_from',           -- Optional: default is 'valid_from'
    valid_to_column='valid_to',               -- Optional: default is 'valid_to'
    is_current_column='is_current',           -- Optional: default is 'is_current'
    control_table_ref='control_table'         -- Optional: default is 'control_table'
) }}

Example with multiple key columns (e.g., for an orders dimension):

-- models/dimensions/dim_order_items_scd2.sql
-- Calls the generic SCD Type 2 merge macro for the order items dimension.

{{ dbt_production_scd2_process.scd_type2_merge_macro(
    source_table_name='order_items_raw',
    source_timestamp_column='item_last_updated_at',
    key_columns=['order_id', 'item_id'],      -- Example: Multiple key columns
    change_columns=[
        'quantity',
        'price_usd',
        'status'
    ],
    valid_from_column='valid_from',
    valid_to_column='valid_to',
    is_current_column='is_current',
    control_table_ref='control_table'
) }}

5. Source Definition (sources.yml)

No change from the previous version.

File: models/sources.yml

# models/sources.yml

version: 2

sources:
  - name: raw # Logical name for your raw data source
    database: your_project_id # Your GCP project ID
    schema: your_raw_dataset_id # The BigQuery dataset where your raw data resides

    tables:
      - name: events # The actual table name in BigQuery (e.g., from your streaming ingestion)
        loaded_at_field: last_updated_at # The timestamp column in your source for freshness checks
        freshness:
          warn_after: {count: 12, period: hour}
          error_after: {count: 24, period: hour}
      # Add other raw source tables here, e.g.:
      # - name: products
      #   loaded_at_field: updated_at
      # - name: order_items_raw # Example for multiple key columns
      #   loaded_at_field: item_last_updated_at

6. dbt_project.yml Configuration

No change from the previous version.

File: dbt_project.yml

# dbt_project.yml

name: 'your_dbt_project' # Make sure this matches your project name used in macro calls
version: '1.0.0'
config-version: 2

profile: 'your_profile' # Ensure this matches your profiles.yml BigQuery connection

# Define the path to your macros
macro-paths: ["macros"]

models:
  your_dbt_project:
    +materialized: view # Default for all models

    control: # Directory for control models
      +schema: control
      +materialized: table

    staging: # Directory for staging models
      +schema: staging
      +materialized: view

    dimensions: # Directory for dimension models
      +schema: analytics
      # The materialization for models in this directory will be set by the macro's config
      # No need to specify incremental here, as the macro handles it.
      # You can still set common configs if needed, but the macro's config takes precedence.

7. Running the Process

The execution remains the same. You run the specific dimension model which in turn calls the macro.

A. Initial Setup (One-Time / Full Refresh):

Create Control Table:

dbt run --select control_table --full-refresh

Manually insert initial watermark for each source you plan to load:
INSERT INTO your_project_id.control.control_table (source_name, last_extracted_timestamp, loaded_by, last_updated_at) VALUES ('events', '1970-01-01 00:00:00', 'manual_init', CURRENT_TIMESTAMP());
(Repeat for other source_names as needed, e.g., 'products', 'order_items_raw' etc.)

Run Specific SCD Type 2 Dimension (Full Load):

dbt run --select dim_customers_scd2 --full-refresh
# And for a new dimension with multiple keys:
# dbt run --select dim_order_items_scd2 --full-refresh

This will:

Call the scd_type2_merge_macro with the respective dimension model.

The macro will build the dimension from scratch.

The post-hook within the macro will update the control_table for the specific source_name.

B. Subsequent Incremental Runs:

dbt run --select dim_customers_scd2
# And for a new dimension with multiple keys:
# dbt run --select dim_order_items_scd2

This will:

Call the scd_type2_merge_macro.

The macro's internal logic will filter the source based on the last_extracted_timestamp for the relevant source_name.

The MERGE statement will apply the SCD Type 2 logic.

The post-hook will update the control_table again.

8. Production Best Practices

All the production best practices mentioned in the previous document still apply and are even more powerful with this generic approach.

This updated generic process now fully supports composite (multiple) key columns, making it even more flexible and robust for a wider range of dimension modeling scenarios in your data warehouse.